{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from einops import repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\pshun\\pyenvs\\transformer310_env\\lib\\site-packages (from opencv-python) (2.0.2)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 180, 180])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "img_path = r\"C:\\Users\\pshun\\Documents\\python\\tkinter_killingtime\\niwatori.jpg\"\n",
    "img = cv2.imread(img_path, -1)\n",
    "img_size = min(img.shape[:2])\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.ToPILImage(), # numpy.ndarray→Pillowオブジェクトに変換                                \n",
    "   #transforms.Resize(img_size), # 画像サイズを256に変換\n",
    "   transforms.CenterCrop(img_size), # 画像サイズ224でセンタークロッピング\n",
    "   transforms.ToTensor(), # torch.Tensorオブジェクト化\n",
    "   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # 正規化\n",
    "])\n",
    "\n",
    "# OpenCVで読み込んだ画像データ(numpy.ndarray)を前処理を適用する\n",
    "img_tensor = preprocess(img) # [ch, img_size, img_size]\n",
    "img_tensor = img_tensor.unsqueeze(0)\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36, 5])\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size * self.grid_size\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 入力xの形状は[batch_size, in_channels, img_size, img_size]\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2) # [batch_size, patch_size, embed_dim]\n",
    "        return x\n",
    "\n",
    "ps = 30\n",
    "embed_dim = 5\n",
    "in_channels = 3\n",
    "num_patches = (img_size//ps)**2\n",
    "pe = PatchEmbedding(img_size, ps, in_channels, embed_dim).forward(img_tensor)\n",
    "print(pe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36, 5])\n",
      "torch.Size([1, 36, 5])\n"
     ]
    }
   ],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, num_patches):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embed\n",
    "    \n",
    "pos_em = PositionEncoding(embed_dim, num_patches)\n",
    "print(pos_em.forward(pe).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
